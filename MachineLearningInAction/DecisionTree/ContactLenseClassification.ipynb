{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Description:\n",
    "> 这个项目学习决策树模型， 首先这个是通过自己编写函数实现决策树，包括以下步骤：\n",
    ">> * 计算给定数据集的香农熵\n",
    ">> * 按照给定特征划分数据\n",
    ">> * 选择最好的数据集划分方式\n",
    ">> * 递归的创建决策树\n",
    ">\n",
    "> 主要是学习决策树构建的内部原理， 这是ID3算法的实现形式，先实现书上的编码过程（基于Python）， 然后尝试用pandas改进，最后通过调包实现决策树模型， 顺便复习机器学习做数据分析的全过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 创建决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 计算给定数据集的香农熵\n",
    "> 思路：\n",
    "> * 获取到训练样本的长度\n",
    "> * 遍历数据集的标签，计算每个标签的个数存放在字典中\n",
    "> * 计算出比例，然后根据公式求解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算给定数据集的香农熵\n",
    "def calcShannonEnt(dataset):\n",
    "    numEntries = len(dataset)     # 样本数量\n",
    "    labelCounts = {}\n",
    "    for featVec in dataset:\n",
    "        currentLabel = featVec[-1]         # 遍历每个样本，获取标签\n",
    "        if currentLabel not in labelCounts.keys():\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1\n",
    "    \n",
    "    shannonEnt = 0.0\n",
    "    for key in labelCounts:\n",
    "        prob = float(labelCounts[key]) / numEntries\n",
    "        shannonEnt -= prob * math.log(prob, 2)\n",
    "    \n",
    "    return shannonEnt\n",
    "\n",
    "# 看看pandas版本\n",
    "# def calcShannonEnt2(dataset):\n",
    "#     numEntries = dataset[dataset.columns[dataset.shape[1]-1]].count()\n",
    "#     labels_num = dataset[dataset.columns[dataset.shape[1]-1]].value_counts()\n",
    "#     prob = labels_num / numEntries\n",
    "#     shannonEnt = np.sum(-(prob * np.log2(prob)))\n",
    "    \n",
    "#     return shannonEnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. 2 按照给定的特征划分数据集\n",
    "> 思路：\n",
    "> 遍历训练样本， 从给定的特征上进行切分样本，去掉该特征，剩余的特征保存形成新的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照给定特征划分数据集\n",
    "def splitDataSet(dataset, axis, value):\n",
    "    retDataSet = []\n",
    "    for featVec in dataset:\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis]\n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 选择最好的数据集划分方式\n",
    "> 思路： 遍历当前特征的唯一属性值， 对每个唯一属性值划分一次数据集， 然后计算新熵值， 并对所有唯一特征值得到的熵求和。 然后用原来计算的香农熵减去这个新的香农熵，看看信息增益增加了多少，选择使得信息增益增加最大的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1   # 获取总的特征数\n",
    "    baseEntropy = calcShannonEnt(dataSet)\n",
    "    bestInfoGain = 0.0\n",
    "    bestFeature = -1\n",
    "    \n",
    "    # 下面开始变量所有特征， 对于每个特征，要遍历所有样本， 根据遍历的样本划分开数据集，然后计算新的香农熵\n",
    "    for i in range(numFeatures):\n",
    "        featList = [example[i] for example in  dataSet]   #  获取遍历特征的这一列数据，接下来进行划分\n",
    "        uniqueVals = set(featList)              # 从列表中创建集合是Python语言得到唯一元素值的最快方法\n",
    "        newEntropy = 0.0\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            prob = len(subDataSet) / float(len(dataSet))\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)\n",
    "        infoGain = baseEntropy - newEntropy\n",
    "        if (infoGain > bestInfoGain):\n",
    "            bastInfoGain = infoGain\n",
    "            bestFeature = i\n",
    "    \n",
    "    return bestFeature   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 递归构建决策树\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 返回最多的那个标签\n",
    "def majorityCnt(classList):\n",
    "    classCount = {}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote] = 0\n",
    "            classCount[vote] += 1\n",
    "    sortedClassCount = sorted(classCount.values(), reverse=True)\n",
    "    \n",
    "    return sortedClassCount[0]\n",
    "\n",
    "# 递归构建决策树\n",
    "def createTree(dataSet, labels):\n",
    "    classList = [example[-1] for example in dataSet]\n",
    "    if classList.count(classList[0]) == len(classList):   # 类别完全相同则停止划分 这种 (1,2,'yes') (3,4,'yes')  \n",
    "        return classList[0]\n",
    "    if len(dataSet[0]) == 1:          # 遍历所有特征时，(1, 'yes') (2, 'no')这种形式 返回出现次数最多的类别\n",
    "        return majorityCnt(classList)\n",
    "    \n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)    # 选择最好的数据集划分方式,返回的是最好特征的下标\n",
    "    bestFeatLabel = labels[bestFeat]         # 获取到那个最好的特征\n",
    "    myTree = {bestFeatLabel:{}}        # 创建一个myTree,保存创建的树的信息\n",
    "    del(labels[bestFeat])          # 从标签中药删除这个选出的最好的特征，下一轮就不用这个特征了\n",
    "    featValues = [example[bestFeat] for example in dataSet]    # 获取到选择的最好的特征的所有取值\n",
    "    uniqueVals = set(featValues)\n",
    "    for value in uniqueVals:\n",
    "        subLabels = labels[:]\n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)  # 这是个字典嵌套字典的形式\n",
    "    \n",
    "    return myTree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 预测分类函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(inputTree, featLabels, testVec):\n",
    "    firstStr = list(inputTree.keys())[0]        # 获取树的根节点\n",
    "    secondDict = inputTree[firstStr]  # 获取到下面的那个字典\n",
    "    featIndex = featLabels.index(firstStr)       # 找到根节点的索引\n",
    "    for key in secondDict.keys():\n",
    "        if testVec[featIndex] == key:\n",
    "            if type(secondDict[key]).__name__ == 'dict':\n",
    "                classLabel = classify(secondDict[key], featLabels, testVec)\n",
    "            else:\n",
    "                classLabel = secondDict[key]\n",
    "    \n",
    "    return classLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "no\n",
      "yes\n",
      "[0]\n",
      "[0]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# 测试函数文件\n",
    "dataSet = [[1, 1,'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n",
    "labels = ['no surfacing', 'flippers']\n",
    "tmplabels = labels.copy()\n",
    "shannonEnt = calcShannonEnt(dataSet)\n",
    "# print(shannonEnt)\n",
    "# print(chooseBestFeatureToSplit(dataSet))\n",
    "# print(splitDataSet(dataSet, 0, 0))\n",
    "mytree = createTree(dataSet, tmplabels)\n",
    "# print(mytree)\n",
    "print(classify(mytree, labels, [1, 0]))\n",
    "print(classify(mytree, labels, [0, 0]))\n",
    "print(classify(mytree, labels, [1, 1]))\n",
    "\n",
    "# 用sklearn包试试\n",
    "X = np.array([[1, 1], [1, 1], [1, 0], [0, 1], [0, 1]])\n",
    "Y = np.array([1, 1, 0, 0, 0])\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, Y)\n",
    "print(model.predict([[1, 0]]))\n",
    "print(model.predict([[0,0]]))\n",
    "print(model.predict([[1,1]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 基于隐形眼镜的例子构造决策树\n",
    "> 隐形眼镜数据集： http://archive.ics.uci.edu/ml/machine-datasets/lenses/\n",
    "> \n",
    "> 这是一个著名的数据集， 它包含很多患者眼部状况的观察条件以及医生推荐的隐形眼镜类型， "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结\n",
    "> * 讲列表中重复的元素去掉的最好的方式就是变成集合，用Set， 得到列表中的唯一值，相当于pandas的unique， np的unique还排序\n",
    "> * 上面这样构造决策树的方式是ID3算法， 就是划分树的时候，最好的特征有几个值，就划分成多少个分支\n",
    "> * 列表的.extend和.append的效果是不一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "# 测试extend和append\n",
    "a = [1, 2, 3]\n",
    "b = [4, 5, 6]\n",
    "#a.append(b)\n",
    "#print(a)       # [1, 2, 3, [4, 5, 6]]\n",
    "a.extend(b)\n",
    "print(a)       #  [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "\n",
    "c = {'a':[1, 2], 'b':[3,4]}\n",
    "print(list(c.keys())[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
