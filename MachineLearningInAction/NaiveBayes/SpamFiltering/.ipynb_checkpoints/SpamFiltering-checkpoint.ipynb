{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Description:\n",
    "> 该项目利用朴素贝叶斯模型实现垃圾邮件的过滤，属于文本分类的范畴，首先我们需要知道，文本分类如何做呢？ \n",
    ">> *  要从文本中获取特征， 需要先拆分文本，根据某个单词是否出现在文本中标记为0或者1， 把文本中的一个个词条转成词向量、\n",
    ">> * 然后在这个基础上构建贝叶斯分类器，进行分类\n",
    ">> * 最后实现这个项目\n",
    ">\n",
    "> 步骤：\n",
    ">> * 准备数据： 从文本中构建词向量（词集模型或者词袋模型）\n",
    ">> * 训练算法： 从词向量计算概率\n",
    ">> * 测试算法： 根据现实情况修改分类器\n",
    "> \n",
    "> 学习Python处理文本的相关知识和朴素贝叶斯里面的伯努利模型和多项式模型的原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re   #  re是Python的正则模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. 准备数据： 从文本中构建词向量\n",
    "> * 2.1 定义创建列表函数，也就是词库\n",
    "> * 2.2 定义词集模型函数\n",
    "> * 2.3 定义词袋模型函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 定义创建列表函数\n",
    "> createVocabList()函数会创建一个包含在所有文档中出现的不重复词的列表, 通常我们理解的词库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dataset):\n",
    "    vocabSet = set([])         #创建一个空集\n",
    "    for document in dataset:\n",
    "        vocabSet = vocabSet | set(document)    #再创建一个空集后，将每篇文档返回的新词集合添加到该集合中，再求两个集合的并集\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 创建词集模型函数setOfWords2Vec()\n",
    "> 该函数的输入参数为词汇表及某个文档， 输出的是文档向量， 每一个元素为1或者0，分别表示词汇表中的单词在输入文档中是否出现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n一定要注意， 把每一篇文档转换成词向量，这里的方式是先要有一个词库包含所有文档里面的单词（不要重复）， 然后遍历每一篇文档，查看词库里面的词\\n在不在文档里面，在，词库的位置标1，否则标0，返回的是这样的一个向量\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0] * len(vocabList)           #函数首先创建一个和词汇表等长的向量，并将其元素都设置为0\n",
    "    #接着，遍历文档中的所有单词，如果出现了词汇表中的单词，则将输出的文档向量中的对应值设为1。\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec\n",
    "\n",
    "\"\"\"\n",
    "一定要注意， 把每一篇文档转换成词向量，这里的方式是先要有一个词库包含所有文档里面的单词（不要重复）， 然后遍历每一篇文档，查看词库里面的词\n",
    "在不在文档里面，在，词库的位置标1，否则标0，返回的是这样的一个向量\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2.3 定义词带模型函数（bag-of-words）\n",
    "> 词袋模型和词集模型的区别是，不仅统计词库里面的每个词在每一篇文档里面有没有出现， 并且还统计那个词在文档里面出现过几次，相应的位置就会是几。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n把每一篇文档转换成词向量，词袋模型做的是，在有词库的基础上， 遍历每一篇文档，查看词库里面的词，在不在文档里面，在还不行，有几个\\n就标1，否则标0，返回的是这样的一个向量\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bagOfWords2VecMN(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "    return returnVec\n",
    "\n",
    "\"\"\"\n",
    "把每一篇文档转换成词向量，词袋模型做的是，在有词库的基础上， 遍历每一篇文档，查看词库里面的词，在不在文档里面，在还不行，有几个\n",
    "就标1，否则标0，返回的是这样的一个向量\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "['garbage', 'food', 'how', 'stupid', 'please', 'ate', 'to', 'dalation', 'quit', 'mr', 'so', 'dog', 'stop', 'buying', 'my', 'park', 'worthless', 'steak', 'has', 'not', 'I', 'cute', 'flea', 'him', 'help', 'problems', 'maybe', 'take', 'love', 'posting', 'is', 'lick']\n",
      "[[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0], [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# 用一个小文本测试一下上面的函数\n",
    "\"\"\"\n",
    "    以在线社区的留言板为例， 为了不影响社区的发展，我们需要屏蔽侮辱性的言论， 所以要构建一个快速过滤器， 如果某个言论出现了侮辱性\n",
    "    的言论， 我们要把它屏蔽掉，所以任务就是判别言论是侮辱性的还是非侮辱性的。 看下面的例子， 1代表侮辱，0代表非侮辱\n",
    "\"\"\"\n",
    "postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "               ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "               ['my', 'dalation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "               ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "               ['mr', 'lick', 'ate', 'my', 'steak', 'how','to', 'stop', 'him'],\n",
    "               ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']\n",
    "              ]\n",
    "\n",
    "print(len(postingList))\n",
    "classVec = [0, 1, 0, 1, 0, 1]\n",
    "\n",
    "myVocabList = createVocabList(postingList)\n",
    "print(myVocabList)\n",
    "\n",
    "text2Vec = []\n",
    "for i in range(len(postingList)):\n",
    "    text2Vec.append(setOfWords2Vec(myVocabList, postingList[i]))\n",
    "    \n",
    "print(text2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. 定义朴素贝叶斯分类器的训练函数\n",
    "> 从词向量计算概率\n",
    ">> 现在已经知道， 一个词是否出现在一篇文档中， 也知道文档所属的类别。 下面就要用贝叶斯公式来计算概率：\n",
    ">> 使用贝叶斯公式， 对每个类计算该值， 然后比较这两个概率值的大小。 \n",
    ">\n",
    "> 伪代码：\n",
    ">> * 计算每个类别中的文档数目\n",
    ">> * 对每篇训练文档：\n",
    ">>> * 对每个类别：\n",
    ">>>> * 如果词条出现在文档中 -> 增加该词条的计数值\n",
    ">>>> * 增加所有词条的计数值\n",
    ">> * 对每个类别：\n",
    ">>> * 对每个词条：\n",
    ">>>> * 将该词条的数目除以总词条数目得到条件概率\n",
    ">> * 返回每个类别的条件概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n这里的思想， 就是看看一共多少篇文档， 然后只计算了1类的先验概率， 至于条件概率，也比较简单， 由于词向量非1即0\\n所以， 如果想算每个特征的条件概率，直接竖着相加，然后除以相应类别的个数\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "函数说明:朴素贝叶斯分类器训练函数\n",
    "trainMatrix--训练文档矩阵，即setOfWords2Vec返回的returnVec构成的矩阵；trainCategory--训练类别标签向量\n",
    "p1Vect--标记为1的类条件概率数组；p0Vect--标记为0的类条件概率数组；pAbusive是标记为1类的先验概率\n",
    "\"\"\"\n",
    "\n",
    "def trainNB(trainMatrix, trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)    # 计算有多少文档\n",
    "    numWords = len(trainMatrix[0])     # 计算词库里面有多少单词\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)    # 标记为1类的先验概率\n",
    "    \n",
    "    \"\"\"\n",
    "    创建numpy数组初始化为1，拉普拉斯平滑。\n",
    "    创建numpy.zeros数组,词条出现数初始化为0。分母初始化为2\n",
    "    \n",
    "    \"\"\"\n",
    "    p0Num = np.ones(numWords)\n",
    "    p1Num = np.ones(numWords)\n",
    "    \n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0    # 防止分母是0\n",
    "    \n",
    "    # 计算条件概率\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]         # 计算\n",
    "            p1Denom += 1\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += 1\n",
    "        \n",
    "    #由于大部分因子都非常小，防止数值下溢得不到正确答案。于是加log计算，可以使得答案不会过小。\n",
    "    p1Vect = np.log(p1Num/p1Denom)          #change to np.log()\n",
    "    p0Vect = np.log(p0Num/p0Denom)          #change to np.log()\n",
    "    \n",
    "    return p0Vect, p1Vect, pAbusive\n",
    "\n",
    "\"\"\"\n",
    "这里的思想， 就是看看一共多少篇文档， 然后只计算了1类的先验概率， 至于条件概率，也比较简单， 由于词向量非1即0\n",
    "所以， 如果想算每个特征的条件概率，直接竖着相加，然后除以相应类别的个数\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "侮辱性的文档占的比例： 0.5\n",
      "侮辱性文章每个特征的条件概率： [-0.91629073 -0.91629073 -1.60943791 -0.22314355 -1.60943791 -1.60943791\n",
      " -0.91629073 -1.60943791 -0.91629073 -1.60943791 -1.60943791 -0.51082562\n",
      " -0.91629073 -0.91629073 -1.60943791 -0.91629073 -0.51082562 -1.60943791\n",
      " -1.60943791 -0.91629073 -1.60943791 -1.60943791 -1.60943791 -0.91629073\n",
      " -1.60943791 -1.60943791 -0.91629073 -0.91629073 -1.60943791 -0.91629073\n",
      " -1.60943791 -1.60943791]\n",
      "非侮辱性文章每个特征的条件概率： [-1.60943791 -1.60943791 -0.91629073 -1.60943791 -0.91629073 -0.91629073\n",
      " -0.91629073 -0.91629073 -1.60943791 -0.91629073 -0.91629073 -0.91629073\n",
      " -0.91629073 -1.60943791 -0.22314355 -1.60943791 -1.60943791 -0.91629073\n",
      " -0.91629073 -1.60943791 -0.91629073 -0.91629073 -0.91629073 -0.51082562\n",
      " -0.91629073 -0.91629073 -1.60943791 -1.60943791 -0.91629073 -1.60943791\n",
      " -0.91629073 -0.91629073]\n"
     ]
    }
   ],
   "source": [
    "# 测验函数\n",
    "p0v, p1v, pAb = trainNB0(text2Vec, classVec)\n",
    "print(\"侮辱性的文档占的比例：\", pAb)\n",
    "\n",
    "print(\"侮辱性文章每个特征的条件概率：\", p1v)\n",
    "print(\"非侮辱性文章每个特征的条件概率：\", p0v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4. 定义朴素贝叶斯分类器的预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#函数说明:朴素贝叶斯分类器分类函数\n",
    "#vec2Classify--待分类的词条数组; p1Vec--标记为类1的类条件概率数组; p0Vec--标记为类0的类条件概率数组; pClass1--标记为1类的先验概率\n",
    "d\n",
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    \"\"\"\n",
    "    1.计算待分类词条数组为1类的概率\n",
    "        这里我感觉vec2Classify是一维数组， 并且这里使用的朴素贝叶斯里面的多项式模型\n",
    "    \"\"\"\n",
    "    #寻找vec2Classify测试数组中，元素为0时对应的索引值\n",
    "    index = np.where(vec2Classify==0)\n",
    "    #遍历元素为0时的索引值，并从p1Vec--1类的条件概率数组中取出对应索引的数值，并存储成列表的形式（p1Vec0=[]）\n",
    "    p1Vec0=[]\n",
    "    for i in index:\n",
    "        for m in i:\n",
    "            p1Vec0.append(p1Vec[m])\n",
    "    #所有1-P(vec2Classify=0|1)组成的列表\n",
    "        x0=np.ones(len(p1Vec0))-p1Vec0\n",
    "    #寻找vec2Classify测试数组中，元素为1时对应的索引值\n",
    "    index1= np.where(vec2Classify==1)\n",
    "    #遍历元素为1时的索引值，并从p1Vec--1类的条件概率数组中取出对应索引的数值，并存储成列表的形式（p1Vec1=[]）\n",
    "    p1Vec1=[]\n",
    "    for i in index1:\n",
    "        for m in i:\n",
    "            p1Vec1.append(p1Vec[m])\n",
    "    #所有P(vec2Classify=0|1)组成的列表\n",
    "    x1=p1Vec1      \n",
    "    ##对应元素相乘。logA * B = logA + logB，所以这里加上log(pClass1)\n",
    "    p1 = sum(x0)+sum(x1) +  np.log(pClass1)        \n",
    "    \"\"\"\n",
    "    2.计算待分类词条数组为0类的概率\n",
    "    \"\"\"\n",
    "    \n",
    "    #寻找vec2Classify测试数组中，元素为0时对应的索引值\n",
    "    index2 = np.where(vec2Classify==0)\n",
    "    #遍历元素为0时的索引值，并从p0Vec--0类的条件概率数组中取出对应索引的数值，并存储成列表的形式（p0Vec0=[]）\n",
    "    p0Vec0=[]\n",
    "    for i in index2:\n",
    "        for m in i:\n",
    "            p0Vec0.append(p0Vec[m])\n",
    "    #所有1-P(vec2Classify=0|0)组成的列表\n",
    "    w0=np.ones(len(p0Vec0))-p0Vec0\n",
    "    #寻找vec2Classify测试数组中，元素为1时对应的索引值\n",
    "    index3= np.where(vec2Classify==1)\n",
    "    #遍历元素为1时的索引值，并从p0Vec--0类的条件概率数组中取出对应索引的数值，并存储成列表的形式（p0Vec1=[]）\n",
    "    p0Vec1=[]\n",
    "    for i in index3:\n",
    "        for m in i:\n",
    "            p0Vec1.append(p0Vec[m])\n",
    "    #所有1-P(vec2Classify=0|0)组成的列表\n",
    "    w1=p0Vec1\n",
    "    ##对应元素相乘。logA * B = logA + logB，所以这里加上log(pClass1)\n",
    "    p0 = sum(w0)+sum(w1) +  np.log(1.0 - pClass1)\n",
    "    \n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 使用朴素贝叶斯过滤垃圾邮件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'该函数textParse()接受一个大字符串并将其解析为字符列表。去掉少于两个字符的字符串， 并将所有字符串为小写'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "书本中4.6.1节 准备数据，切分文本部分写的很清晰。\n",
    "\"\"\"\n",
    "#将一个大字符串解析为字符列表。input is big string, #output is word list\n",
    "def textParse(bigString):    \n",
    "    listOfTokens = re.split(r'\\W+', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n",
    "\n",
    "\"\"\"该函数textParse()接受一个大字符串并将其解析为字符列表。去掉少于两个字符的字符串， 并将所有字符串为小写\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaa', 'bbb', 'ccc', 'ddd', 'eee', 'fff']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = \"aaa bbb ccc; ddd eee; a fff\"\n",
    "textParse(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spamTest():\n",
    "    docList = []; classList = []; fullText = []\n",
    "    #遍历25个txt文件\n",
    "    for i in range(1, 26):\n",
    "        #读取每个垃圾邮件，大字符串转换成字符列表。\n",
    "        wordList = textParse(open('email/spam/%d.txt' % i, encoding=\"ISO-8859-1\").read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        #标记垃圾邮件，1表示垃圾邮件\n",
    "        classList.append(1)\n",
    "        #读取每个非垃圾邮件，字符串转换为字符列表\n",
    "        wordList = textParse(open('email/ham/%d.txt' % i, encoding=\"ISO-8859-1\").read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        #标记每个非垃圾邮件，0表示非垃圾邮件\n",
    "        classList.append(0)\n",
    "    #创建词汇表，不重复\n",
    "    vocabList = createVocabList(docList)\n",
    "    #创建存储训练集的索引值的列表\n",
    "    trainingSet =list(range(50));          # 0-49的列表\n",
    "    #创建存储测试集的索引值的列表\n",
    "    testSet= [] \n",
    "    #从50个邮件中，随机挑选出40个作为训练集，10个作为测试集\n",
    "    for i in range(10):\n",
    "        #随机选取索引值\n",
    "        randIndex = int(np.random.uniform(0, len(trainingSet)))      # 随机选择10个\n",
    "        #添加测试集的索引值\n",
    "        testSet.append(trainingSet[randIndex])              #  得到测试集\n",
    "        #在训练集的列表中删除添加到测试集的索引值\n",
    "        del(list(trainingSet)[randIndex])     # 从训练集中删除\n",
    "    #创建训练集矩阵和训练集类别标签向量\n",
    "    trainMat = []; \n",
    "    trainClasses = []\n",
    "    #遍历训练集，目前只有40个训练集\n",
    "    for docIndex in trainingSet:\n",
    "        #将生成的词集模型添加到训练矩阵中\n",
    "        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n",
    "        #将类别标签添加到训练集的类别标签向量中\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    \"\"\"\n",
    "    训练朴素贝叶斯模型\n",
    "    \"\"\"\n",
    "    #训练朴素贝叶斯模型\n",
    "    p0V, p1V, pSpam = trainNB(np.array(trainMat), np.array(trainClasses))\n",
    "    #错误分类计数\n",
    "    errorCount = 0\n",
    "    #遍历测试集\n",
    "    for docIndex in testSet:    \n",
    "        #测试集的词集模型\n",
    "        wordVector = setOfWords2Vec(vocabList, docList[docIndex])\n",
    "        if classifyNB(np.array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "            print(\"classification error\", docList[docIndex])\n",
    "    print('the error rate is: ', float(errorCount)/len(testSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification error ['benoit', 'mandelbrot', '1924', '2010', 'benoit', 'mandelbrot', '1924', '2010', 'wilmott', 'team', 'benoit', 'mandelbrot', 'the', 'mathematician', 'the', 'father', 'fractal', 'mathematics', 'and', 'advocate', 'more', 'sophisticated', 'modelling', 'quantitative', 'finance', 'died', '14th', 'october', '2010', 'aged', 'wilmott', 'magazine', 'has', 'often', 'featured', 'mandelbrot', 'his', 'ideas', 'and', 'the', 'work', 'others', 'inspired', 'his', 'fundamental', 'insights', 'you', 'must', 'logged', 'view', 'these', 'articles', 'from', 'past', 'issues', 'wilmott', 'magazine']\n",
      "classification error ['thanks', 'peter', 'definitely', 'check', 'this', 'how', 'your', 'book', 'going', 'heard', 'chapter', 'came', 'and', 'was', 'good', 'shape', 'hope', 'you', 'are', 'doing', 'well', 'cheers', 'troy']\n",
      "classification error ['arvind', 'thirumalai', 'commented', 'your', 'status', 'arvind', 'wrote', 'you', 'know', 'reply', 'this', 'email', 'comment', 'this', 'status']\n",
      "the error rate is:  0.3\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 总结\n",
    "> * np.where对于二维数组是啥样子？\n",
    "> * [正则式的几种表达](https://blog.csdn.net/a18852867035/article/details/82846763)\n",
    "> * [re.split和str.split](https://www.jianshu.com/p/5c8c7e38891c)\n",
    "> * 关于朴素贝叶斯， 后面基于三种分布，高斯分布， 多项式分布和伯努利分布， 在自然语言处理问题中，多项式分布和伯努利分布有点差别， 参考下面的两篇博客：\n",
    ">> * [机器学习之基于朴素贝叶斯文本分类算法](https://blog.csdn.net/lming_08/article/details/37542331)\n",
    ">> * [朴素贝叶斯的三个常用模型：高斯、多项式、伯努利](https://blog.csdn.net/qq_27009517/article/details/80044431)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "(array([0, 1, 2], dtype=int64), array([1, 1, 2], dtype=int64))\n",
      "[0 1 2]\n",
      "0\n",
      "1\n",
      "2\n",
      "[1 1 2]\n",
      "1\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1, 2, 3], [4, 2, 6], [7, 8, 2]])\n",
    "index = np.where(a==2)\n",
    "print(type(index))\n",
    "print(index)\n",
    "list(index)\n",
    "for i in index:\n",
    "    print(i)\n",
    "    for m in i:\n",
    "        print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaa', 'bbb', 'ccc', 'ddd', 'eee', 'fff']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = \"aaa bbb ccc; ddd eee;fff\"\n",
    "re.split(r'\\W+', line)\n",
    "\n",
    "\"\"\"W+是这直接按照能分隔的字符进行分隔\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
